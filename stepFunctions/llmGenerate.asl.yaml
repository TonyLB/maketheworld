---
Comment: Command step-function to generate LLM outputs
StartAt: Define Defaults
States:
    "Define Defaults":
        Type: Pass
        Next: Apply Defaults
        ResultPath: "$.inputDefaults"
        Parameters:
            connectionId: ""
            requestId: ""
    "Apply Defaults":
        Type: Pass
        Next: Check Name
        ResultPath: "$"
        Parameters:
            "args.$": "States.JsonMerge($.inputDefaults, $$.Execution.Input, false)"
    "Check Name":
        Type: Choice
        Choices:
            -
                Variable: "$.args.name"
                IsPresent: True
                Next: Create Prompt
        Default: Fail
    "Create Prompt":
        Type: Task
        Next: Invoke Model
        Resource: "${LargeLanguageModelFunctionArn}"
        Parameters:
            type: RoomGenerate
            "name.$": "$.args.name"
        Catch:
            -
                ErrorEquals:
                    - States.ALL
                Next: Fail
        ResultPath: "$.results"
    "Invoke Model":
        Type: Task
        Resource: "arn:aws:states:::bedrock:invokeModel"
        Parameters:
            ModelId: "arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1"
            Body:
                "inputText.$": "$.results.prompt"
                textGenerationConfig:
                    maxTokenCount: 300
                    temperature: 0.5
                    topP: 0.9
        OutputPath: "$.Body"
        Next: Done
    Fail:
        Type: Fail
    Done:
        Type: Succeed
